{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate CRF models for person names, orgs and locations using the Presidio Evaluator framework\n",
    "\n",
    "Data = `generated_test_November 12 2019`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "import logging\n",
    "from presidio_evaluator import InputSample\n",
    "from presidio_evaluator.data_generator import read_synth_dataset\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "pd.set_option('display.width', 10000)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select data for evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "synth_samples = read_synth_dataset(\"../../data/synth_dataset.txt\")\n",
    "print(len(synth_samples))\n",
    "\n",
    "\n",
    "DATASET = synth_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "entity_counter = Counter()\n",
    "for sample in DATASET:\n",
    "    for tag in sample.tags:\n",
    "        entity_counter[tag]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "entity_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "DATASET[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "#max length sentence\n",
    "max([len(sample.tokens) for sample in DATASET])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select models for evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "crf_vanilla = \"../../model-outputs/crf.pickle\"\n",
    "    \n",
    "models = [crf_vanilla]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run evaluation on all models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from presidio_evaluator.crf_evaluator import CRFEvaluator\n",
    "\n",
    "for model in models:\n",
    "    print(\"-----------------------------------\")\n",
    "    print(\"Evaluating model {}\".format(model))\n",
    "    crf_evaluator = CRFEvaluator(model_pickle_path=model)\n",
    "    evaluation_results = crf_evaluator.evaluate_all(DATASET)\n",
    "    scores = crf_evaluator.calculate_score(evaluation_results)\n",
    "    \n",
    "    print(\"Confusion matrix:\")\n",
    "    print(scores.results)\n",
    "\n",
    "    print(\"Precision and recall\")\n",
    "    scores.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom evaluation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out the model\n",
    "def sent_to_features(model_path,sent):\n",
    "    \"\"\"\n",
    "    Translates a sentence into a prediction using a saved CRF model\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    \n",
    "    tokenizer = spacy.blank('en')\n",
    "    tokens = tokenizer(sent)\n",
    "    tags = ['O' for token in tokens] # Placeholder: Not used but required. \n",
    "    metadata = {'Template#':1,'Gender':'1','Country':'2'} #Placeholder: Not used but required\n",
    "    input_sample = InputSample(full_text=sent,masked=\"\",spans=None,tokens=tokens,tags=tags,metadata=metadata,create_tags_from_span=False,)\n",
    "\n",
    "    return CRFEvaluator.crf_predict(input_sample, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE = \"Michael is American\"\n",
    "\n",
    "sent_to_features(model_path=crf_vanilla, sent=SENTENCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### False positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Most false positive tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "errors = scores.model_errors\n",
    "\n",
    "from presidio_evaluator import ModelEvaluator\n",
    "ModelEvaluator.most_common_fp_tokens(errors)#[model_error for model_error in errors if model_error.error_type =='FP']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. review false positives for entity 'PERSON'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "fps_df = ModelEvaluator.get_fps_dataframe(errors,entity='PERSON')\n",
    "fps_df[['full_text','token','prediction']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### False negative examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "ModelEvaluator.most_common_fn_tokens(errors,n=50, entity='PERSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More FN analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "fns_df = ModelEvaluator.get_fns_dataframe(errors,entity='PERSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "fns_df[['full_text','token','annotation','prediction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}