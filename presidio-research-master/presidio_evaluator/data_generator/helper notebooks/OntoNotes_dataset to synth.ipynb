{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes the ontonoes ner dataset, and creates templates (utterances with placeholders) for a PII synthetic data generator to use in order to create new sentences.\n",
    "\n",
    "The notebook additionally introduces two new entities: TITLE and ROLE, in order to overcome cases like \"UK David Scott called his wife\", where the original sentence is \"UK Prime Minister Boris Johnson called his wife\" as \"Prime Minister\" was originally tagged as PER in the original dataset. Same logic goes for titles, like Mr., Mrs., Ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_rows = 4000\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download OntoNotes data\n",
    "ontonotes = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To pandas + add sentence_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "sentence_id = 0\n",
    "for sentence in ontonotes:\n",
    "   \n",
    "    df = pd.DataFrame(sentence,columns = [\"word\",\"tag\"])\n",
    "    df[\"sentence_idx\"] = sentence_id\n",
    "    sentence_id+=1\n",
    "    df_list.append(df)\n",
    "ner_dataset = pd.concat(df_list)\n",
    "ner_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ner_dataset.groupby('sentence_idx')['word'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sentences[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset[ner_dataset['sentence_idx']==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique entities\n",
    "ner_dataset['tag'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace tokenization replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset['word'] = ner_dataset['word']\\\n",
    ".replace('-LRB-','(')\\\n",
    ".replace('-RRB-',')')\\\n",
    ".replace('-LCB-','(')\\\n",
    ".replace('-RCB-',')')\\\n",
    ".replace('``','\"')\\\n",
    ".replace(\"''\",'\"')\\\n",
    ".replace('/.','.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper columns:\n",
    "ner_dataset['prev-word'] = ner_dataset.word.shift(1)\n",
    "ner_dataset['prev-prev-word'] = ner_dataset['word'].shift(2)\n",
    "ner_dataset['next-word'] = ner_dataset['word'].shift(-1)\n",
    "ner_dataset['next-next-word'] = ner_dataset['word'].shift(-2)\n",
    "ner_dataset['prev-tag'] = ner_dataset['tag'].shift(1)\n",
    "ner_dataset['next-tag'] = ner_dataset['tag'].shift(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove unneeded (non PII) entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAGS_TO_IGNORE = ['CARDINAL','FAC','LAW','LANGUAGE','TIME','DATE','ORDINAL','EVENT','QUANTITY','WORK_OF_ART','MONEY','PRODUCT','PERCENT']\n",
    "def remote_unwanted_tags(x):\n",
    "    if len(x)>1 and x[2:] in TAGS_TO_IGNORE:\n",
    "        return 'O'\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "ner_dataset['tag'] = ner_dataset['tag'].apply(remote_unwanted_tags)\n",
    "ner_dataset[ner_dataset['sentence_idx']==3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove PERSON tags if preceding word is 'the' (e.g. the Bush administration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing PERSON tags from sentences with a 'the' preceding the person:\n",
    "\n",
    "def remove_tag_if_the_person(row):\n",
    "    if row['prev-word'].lower() == 'the' and row['tag']=='B-PERSON':\n",
    "        return 'O'\n",
    "    elif row['prev-prev-word'].lower() == 'the' and row['prev-tag']=='I-PERSON' and row['tag']=='B-PERSON':\n",
    "        return 'O'\n",
    "    return row['tag']\n",
    "\n",
    "ner_dataset['prev-word']=ner_dataset['prev-word'].astype('str')\n",
    "ner_dataset['prev-prev-word']=ner_dataset['prev-prev-word'].astype('str')\n",
    "ner_dataset['tag'] = ner_dataset.apply(remove_tag_if_the_person,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove tag from 's (Joe Wilson's cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tag_if_apostraphe_after_tag(row):\n",
    "    if row['prev-tag'] != 'O' and row['word']==\"'s\":\n",
    "        return 'O'\n",
    "    return row['tag']\n",
    "ner_dataset['tag'] = ner_dataset.apply(remove_tag_if_the_person,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-tag words from dictionaries (countries, nationalities, roles, titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nationalities and countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "nationalities = pd.read_csv(\"../raw_data/nationalities.csv\")\n",
    "nationalities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"algeria\" in nationalities['country'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ner_dataset['metadata'] = None\n",
    "\n",
    "def get_nationality_as_metadata(row):\n",
    "    if row['word'].lower() in nationalities['country'].values:\n",
    "        return 'COUNTRY'\n",
    "    elif row['word'].lower() in nationalities['nationality'].values:\n",
    "        return 'NATIONALITY'\n",
    "    elif row['word'].lower() in nationalities['man'].values:\n",
    "        return 'NATION_MAN'\n",
    "    elif row['word'].lower() in nationalities['woman'].values:\n",
    "        return 'NATION_WOMAN'\n",
    "    elif row['word'].lower() in nationalities['plural'].values:\n",
    "        return 'NATION_PLURAL'\n",
    "    return row['metadata']\n",
    "\n",
    "row = pd.Series({'word':'Frenchwoman','metadata':None})\n",
    "print(\"Example: Frenchwoman -> \",get_nationality_as_metadata(row))\n",
    "\n",
    "def update_tag_based_on_metadata(row):\n",
    "    if row['tag'] != 'O' and row['metadata'] is not None:\n",
    "        return row['tag'][:2] + row['metadata']\n",
    "    else:\n",
    "        return row['tag']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset['metadata'] = ner_dataset.apply(get_nationality_as_metadata, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "MALE_TITLES = ['mr', 'dr', 'professor', 'eng','prof','doctor']\n",
    "FEMALE_TITLES = ['mrs', 'ms', 'miss', 'dr', 'professor', 'eng', 'prof','doctor']\n",
    "\n",
    "def get_title_as_metadata(row):\n",
    "    if row['word'].lower() in MALE_TITLES:\n",
    "        return 'MALE_TITLE'\n",
    "    elif row['word'].lower() in FEMALE_TITLES:\n",
    "        return 'FEMALE_TITLE'\n",
    "    return row['metadata']\n",
    "\n",
    "\n",
    "def update_title_tag_if_missing(row):\n",
    "    if row['word'].lower() in MALE_TITLES and row['tag']=='O':\n",
    "        return 'B-MALE_TITLE'\n",
    "    elif row['word'].lower() in FEMALE_TITLES and row['tag']=='O':\n",
    "        return 'B-FEMALE_TITLE'\n",
    "    else:\n",
    "        return row['tag']\n",
    "\n",
    "ner_dataset['metadata'] = ner_dataset.apply(get_title_as_metadata,axis=1)\n",
    "ner_dataset['tag'] = ner_dataset.apply(update_title_tag_if_missing,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset[ner_dataset['sentence_idx']==18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove 'the' from 'the NORP' if NORP is not in nationalities list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tag_if_the_norp(row):\n",
    "    if row['prev-word'].lower() == 'the' and row['tag']=='B-NORP' and row['metadata'] is None:\n",
    "        return 'O'\n",
    "    elif row['prev-prev-word'].lower() == 'the' and row['prev-tag']=='I-NORP' and row['tag']=='B-NORP' and row['metadata'] is None:\n",
    "        return 'O'\n",
    "    return row['tag']\n",
    "ner_dataset['tag'] = ner_dataset.apply(remove_tag_if_the_norp,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove sentences with adjacent different entities (e.g calling from New York Larry King)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset['entity'] = ner_dataset['tag'].str[2:]\n",
    "ner_dataset['next-entity']=ner_dataset['next-tag'].str[2:]\n",
    "adjacent_idc = (ner_dataset['tag'] != 'O') & (ner_dataset['next-tag'] != 'O') & (ner_dataset['entity'] != ner_dataset['next-entity'])\n",
    "sentences_to_remove = ner_dataset[adjacent_idc]['sentence_idx'].values\n",
    "sentences_to_remove\n",
    "\n",
    "ner_dataset=ner_dataset[~ner_dataset['sentence_idx'].isin(sentences_to_remove)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update tag for discovered metadata values (eg. nationalities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset['tag'] = ner_dataset.apply(update_tag_based_on_metadata, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create templates base on NER dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        self.n_sent = 1\n",
    "        self.dataset = dataset\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t) for w,t in zip(s[\"word\"].values.tolist(),\n",
    "                                                        s[\"tag\"].values.tolist())]\n",
    "        self.grouped = self.dataset.groupby(\"sentence_idx\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    @staticmethod    \n",
    "    def cleanse_template(template, ents):\n",
    "        # Remove whitespace before certain punctuation marks\n",
    "        template = re.sub(r'\\s([?,:.!](?:|$))+', r'\\1', template)\n",
    "        \n",
    "        # Remove whitespaces within double quotes\n",
    "        template = re.sub('\\\"\\s*([^\\\"]*?)\\s*\\\"', r'\"\\1\"', template)    \n",
    "        \n",
    "        # Remove whitespaces within quotes\n",
    "        template = re.sub(\"\\'\\s*([^\\']*?)\\s*\\'\", r\"'\\1'\", template)    \n",
    "        \n",
    "        # Remove whitespaces within parentheses\n",
    "        template = re.sub('\\(\\s*([^\\(]*?)\\s*\\)', r'(\\1)', template)    \n",
    "        \n",
    "        for ent in ents:\n",
    "            #Turn PERSON PERSON into PERSON\n",
    "            duplicates = \"[{}] [{}]\".format(ent,ent)\n",
    "            template = template.replace(duplicates,\"[{}]\".format(ent))\n",
    "        \n",
    "        \n",
    "        # Replace additional weird templates:\n",
    "        to_replace = {\n",
    "            \"[LOCATION] says\" : \"[PERSON] says\",\n",
    "            \"[LOCATION] said\" : \"[PERSON] said\",\n",
    "            \"[ORGANIZATION] of [ORGANIZATION]\" : \"[ORGANIZATION]\",\n",
    "            \"the [COUNTRY]\" : \"[COUNTRY]\",\n",
    "            \" 's \":\"'s\",\n",
    "            \"] 's \":\"]'s \",\n",
    "            \"] 's,\":\"]'s,\",\n",
    "            \"] 's.\":\"]'s.\",\n",
    "            \" n't\" : \"n't\",\n",
    "            \"/?\":\"?\",\n",
    "            \"%u\":\"u\",\n",
    "            \"%m\":\"m\",\n",
    "            \"%e\":\"e\",  \n",
    "            \"%h\":\"h\",  \n",
    "            \"%a\":\"a\",\n",
    "            \" %\":\"%\",\n",
    "            \" ?\":\"?\",\n",
    "            \" /?\":\"?\",\n",
    "            \" ' .\":\"'.\",\n",
    "            \"[ \":\"(\",\n",
    "            \" ]\":\")\",\n",
    "            \"[PERSON] -- [PERSON]\":\"[PERSON]\",\n",
    "            \"[COUNTRY] -- [ORGANIZATION]\":\"[ORGANIZATION]\",\n",
    "            \"Jews\" : \"[NATIONALITY]\",\n",
    "            \"Chinese\" : \"[NATIONALITY]\",\n",
    "            \"Dutch\" : \"[NATIONALITY]\",\n",
    "            \"[LOCATION], [LOCATION]\":\"[LOCATION]\"\n",
    "        }\n",
    "        \n",
    "        for weird in to_replace.keys():\n",
    "            #if weird in template:\n",
    "            #    print(\"Weird sentence\",template)\n",
    "            template = template.replace(weird,to_replace[weird])\n",
    "  \n",
    "        template = template.replace(\" -- \",\" - \")\n",
    "        \n",
    "        #Ignore templates that are incomplete\n",
    "        if \"/-\" in template:\n",
    "            template = \"\"\n",
    "            \n",
    "        if template.count('\"') == 1:\n",
    "            template = template.replace('\"','')\n",
    "\n",
    "        return template\n",
    "    \n",
    "    @staticmethod    \n",
    "    def get_template(grouped,entity_name_replace_dict):\n",
    "        template = \"\"\n",
    "        i=0\n",
    "        cur_index = 0\n",
    "        ents = []\n",
    "        for token in grouped:\n",
    "            # remove brackets as they interefere with the data generation process\n",
    "            token_text = token[0].replace(\"[\", \"(\").replace(\"]\",\")\")\n",
    "            token_text = token[0].replace(\"{\", \"(\").replace(\"}\",\")\")\n",
    "            token_tag = token[1]\n",
    "            token_entity = token_tag[2:] if len(token_tag)>1 else token_tag\n",
    "            \n",
    "            if token_entity == 'O':\n",
    "                template += \" \" + token_text\n",
    "            elif 'B-' in token_tag and token_entity not in TAGS_TO_IGNORE:\n",
    "                #print(\"found entity: {}\".format(token_entity))\n",
    "                ent = entity_name_replace_dict[token_entity]\n",
    "                ents.append(ent)\n",
    "                 \n",
    "                template += \" [\" + ent + \"]\"\n",
    "            #print(\"template: \",template)\n",
    "        \n",
    "        template = SentenceGetter.cleanse_template(template, ents)\n",
    "        \n",
    "        return template.strip()\n",
    "    \n",
    "getter = SentenceGetter(ner_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITIES_DICTIONARY = {\"PERSON\":\"PERSON\",\n",
    "                       \"GPE\":\"COUNTRY\",\n",
    "                       \"NORP\":\"LOCATION\",\n",
    "                       \"LOC\":\"LOCATION\",\n",
    "                       \"ORG\":\"ORGANIZATION\",\n",
    "                       \"MALE_TITLE\":\"MALE_TITLE\",\n",
    "                       \"FEMALE_TITLE\":\"FEMALE_TITLE\",\n",
    "                       \"COUNTRY\":\"COUNTRY\",\n",
    "                       \"NATIONALITY\":\"NATIONALITY\",\n",
    "                       \"NATION_WOMAN\":\"NATION_WOMAN\",\n",
    "                       \"NATION_MAN\":\"NATION_MAN\",\n",
    "                       \"NATION_PLURAL\":\"NATION_PLURAL\"}\n",
    "                      \n",
    "\n",
    "\n",
    "sentences = getter.sentences\n",
    "\n",
    "sent_id = 445\n",
    "\n",
    "print(\"original:\",sentences[sent_id])\n",
    "print(\"template:\", getter.get_template(sentences[sent_id],entity_name_replace_dict=ENTITIES_DICTIONARY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_templates = [getter.get_template(sentence,entity_name_replace_dict=ENTITIES_DICTIONARY) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"original length of templates: {}\".format(len(all_templates)))\n",
    "all_templates = list(set(all_templates))\n",
    "print(\"length after duplicates removal: {}\".format(len(all_templates)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file\n",
    "\n",
    "with open(\"../raw_data/ontonotes_based_templates.txt\",\"w+\",encoding='utf-8') as f:\n",
    "    for template in all_templates:\n",
    "        f.write(\"%s\\n\" % template)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"[NATIONALITY]'s[MALE_TITLE]'\"\n",
    "\n",
    "template = getter.cleanse_template(template,[])\n",
    "#template = re.sub('\\(\\s*([^\\(]*?)\\s*\\)', r'(\\1)', template)    \n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "if template.count(\"'\")==1:\n",
    "    print(True)\n",
    "    template = template.replace(\"'\",'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}