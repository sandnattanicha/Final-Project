{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory data analysis on the OntoNotes dataset, to gain insights towards the templating of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.max_rows = 4000\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll = \"\" # Download CoNLL-2003\n",
    "\n",
    "df_list = []\n",
    "sentence_id = 0\n",
    "for sentence in conll:\n",
    "   \n",
    "    df = pd.DataFrame(sentence,columns = [\"word\",\"tag\"])\n",
    "    df[\"sentence_idx\"] = sentence_id\n",
    "    sentence_id+=1\n",
    "    df_list.append(df)\n",
    "ner_dataset = pd.concat(df_list)\n",
    "ner_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAGS_TO_IGNORE = ['CARDINAL','FAC','LAW','LANGUAGE','TIME','DATE','ORDINAL','EVENT','QUANTITY','WORK_OF_ART','MONEY','PRODUCT','PERCENT']\n",
    "def remote_unwanted_tags(x):\n",
    "    if len(x)>1 and x[2:] in TAGS_TO_IGNORE:\n",
    "        return 'O'\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "ner_dataset['tag'] = ner_dataset['tag'].apply(remote_unwanted_tags)\n",
    "ner_dataset[ner_dataset['sentence_idx']==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ner_dataset.groupby('sentence_idx')['word'].transform(lambda x: ' '.join(x)).unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentences)\n",
    "#print(sentences[:5])\n",
    "with open(\"raw_sentences.txt\",\"w\",encoding=\"utf8\") as f:\n",
    "    for item in sentences:\n",
    "        f.write(\"{}\\n\".format(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of labels per tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset.groupby('tag')['tag'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset['word'] = ner_dataset['word'].replace('-LRB-',')')\\\n",
    ".replace('-RRB-',')')\\\n",
    ".replace('``',\"\\\"\")\\\n",
    ".replace(\"''\",'\"')\\\n",
    ".replace('/.','.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(ner_dataset['word']).most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add lead and lag words and tags to dataset_no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punct = [c for c in string.punctuation]\n",
    "punct.extend([\"--\",\"''\",\"/.\"])\n",
    "print(punct)\n",
    "dataset_no_punct = ner_dataset[~ner_dataset.word.str.strip().isin(punct)]\n",
    "dataset_no_punct['prev-word'] = dataset_no_punct.word.shift(1)\n",
    "dataset_no_punct['prev-prev-word'] = dataset_no_punct['word'].shift(2)\n",
    "dataset_no_punct['next-word'] = dataset_no_punct['word'].shift(-1)\n",
    "dataset_no_punct['prev-tag'] = dataset_no_punct['tag'].shift(1)\n",
    "dataset_no_punct['next-tag'] = dataset_no_punct['tag'].shift(-1)\n",
    "dataset_no_punct.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add features for easier manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset['prev-word'] = ner_dataset.word.shift(1)\n",
    "ner_dataset['prev-prev-word'] = ner_dataset['word'].shift(2)\n",
    "ner_dataset['next-word'] = ner_dataset['word'].shift(-1)\n",
    "ner_dataset['next-next-word'] = ner_dataset['word'].shift(-2)\n",
    "ner_dataset['prev-tag'] = ner_dataset['tag'].shift(1)\n",
    "ner_dataset['next-tag'] = ner_dataset['tag'].shift(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gather statistics on the first person token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "bper = dataset_no_punct[dataset_no_punct['tag']=='B-PERSON']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of B-PERSON tokens\n",
    "from collections import Counter\n",
    "Counter(bper['word']).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_bper_token = bper['prev-word'].str.lower()\n",
    "Counter(prev_bper_token).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_prev_bper_token = bper['prev-prev-word']\n",
    "two_prev_tokens = zip(prev_prev_bper_token.str.lower(), prev_bper_token.str.lower())\n",
    "Counter(two_prev_tokens).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find \"the\" followed by B-PERSON\n",
    "the_PERSON = ner_dataset[(ner_dataset['prev-word'].str.lower()==\"the\") & (ner_dataset['tag']=='B-PERSON')]\n",
    "print(the_PERSON['prev-word']+\" \"+the_PERSON['word']+\" \"+the_PERSON['next-word']+\" \"+the_PERSON['next-next-word'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add metadata for nationalities (to differentiate between America, Americans and US citizen)\n",
    "nationalities = pd.read_csv(\"../raw_data/nationalities.csv\")\n",
    "nationalities.head()\n",
    "\n",
    "ner_dataset['metadata'] = None\n",
    "\n",
    "def get_nationality_as_metadata(row):\n",
    "    if row['word'].lower() in nationalities['country'].values:\n",
    "        return 'COUNTRY'\n",
    "    elif row['word'].lower() in nationalities['nationality'].values:\n",
    "        return 'NATIONALITY'\n",
    "    elif row['word'].lower() in nationalities['man'].values:\n",
    "        return 'NATION_MAN'\n",
    "    elif row['word'].lower() in nationalities['woman'].values:\n",
    "        return 'NATION_WOMAN'\n",
    "    return row['metadata']\n",
    "\n",
    "row = pd.Series({'word':'Frenchwoman','metadata':None})\n",
    "print(\"Example: Frenchwoman -> \",get_nationality_as_metadata(row))\n",
    "\n",
    "ner_dataset['metadata'] = ner_dataset.apply(get_nationality_as_metadata, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing PERSON tags from sentences with a 'the' preceding the person:\n",
    "\n",
    "def remove_tag_if_the_person(row):\n",
    "    if row['prev-word'].lower() == 'the' and row['tag']=='B-PERSON':\n",
    "        return 'O'\n",
    "    elif row['prev-prev-word'].lower() == 'the' and row['prev-tag']=='I-PERSON' and row['tag']=='B-PERSON':\n",
    "        return 'O'\n",
    "    return row['tag']\n",
    "\n",
    "def remove_tag_if_the_norp(row):\n",
    "    if row['prev-word'].lower() == 'the' and row['tag']=='B-NORP' and row['metadata'] is None:\n",
    "        return 'O'\n",
    "    elif row['prev-prev-word'].lower() == 'the' and row['prev-tag']=='I-NORP' and row['tag']=='B-NORP' and row['metadata'] is None:\n",
    "        return 'O'\n",
    "    return row['tag']\n",
    "\n",
    "ner_dataset['prev-word']=ner_dataset['prev-word'].astype('str')\n",
    "ner_dataset['prev-prev-word']=ner_dataset['prev-prev-word'].astype('str')\n",
    "ner_dataset['tag'] = ner_dataset.apply(remove_tag_if_the_person,axis=1)\n",
    "ner_dataset['tag'] = ner_dataset.apply(remove_tag_if_the_norp,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find \"the\" followed by B-NORP\n",
    "the_NORP = ner_dataset[(ner_dataset['prev-word'].str.lower()==\"the\") & (ner_dataset['tag']=='B-NORP')]\n",
    "print(the_NORP['prev-word']+\" \"+the_NORP['word']+\" \"+the_NORP['next-word']+\" \"+the_NORP['next-next-word'].values + \" (\" + the_NORP['metadata'] + \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tag_if_apostraphe_after_tag(row):\n",
    "    if row['prev-tag'] != 'O' and row['word']==\"'s\":\n",
    "        return 'O'\n",
    "    return row['tag']\n",
    "ner_dataset['tag'] = ner_dataset.apply(remove_tag_if_apostraphe_after_tag,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_with_president=ner_dataset[ner_dataset['word'].str.lower() == 'president']['sentence_idx']\n",
    "ner_dataset[ner_dataset['sentence_idx']==sentences_with_president.iloc[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset[ner_dataset['tag']=='B-PERSON']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjacent tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset['entity'] = ner_dataset['tag'].str[2:]\n",
    "ner_dataset['next-entity']=ner_dataset['next-tag'].str[2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacent_idc = (ner_dataset['tag'] != 'O') & (ner_dataset['next-tag'] != 'O') & (ner_dataset['entity'] != ner_dataset['next-entity'])\n",
    "print(\"sentences with duplicate different entities: \",str(len(ner_dataset[adjacent_idc])))\n",
    "ner_dataset[adjacent_idc]['sentence_idx']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset[ner_dataset['sentence_idx']==8759]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NORP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "norp_values = ner_dataset[ner_dataset['entity']=='NORP']['word']\n",
    "Counter(norp_values).most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The country?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_X_idx = (ner_dataset['prev-word']=='the') & (ner_dataset['tag'] != 'O')\n",
    "the_X_sentences = ner_dataset[the_X_idx]['sentence_idx']\n",
    "the_X_sentences.values[0]\n",
    "ner_dataset[ner_dataset['sentence_idx']==the_X_sentences.values[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}