{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate new examples based on this dataset: \n",
    "https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus\n",
    "\n",
    "This notebook takes the ner dataset from the previous link, and creates templates (utterances with placeholders) for a PII synthetic data generator to use in order to create new sentences.\n",
    "Note that due to the nature of the tagging, there might be weird output sentences. For example:\n",
    "\n",
    "- The same entity shows multiple times in sentence: \"I travel from Argentina to Argentina\"\n",
    "- Bad grammer due to the lack of inflection and changes to nouns due to context: \"*The statement said no Denmark or India-led troops were killed*\" instead of \"*The statement said no Danish or Indian led troops were killed*\"\n",
    "- Unrealistic sentences due to change in entities: \"Prime minister Lebron James enters the government building in Kuala Lumpur\"\n",
    "\n",
    "\n",
    "The notebook additionally introduces two new entities: TITLE and ROLE, in order to overcome cases like \"UK David Scott called his wife\", where the original sentence is \"UK Prime Minister Boris Johnson called his wife\" as \"Prime Minister\" was originally tagged as PER in the original dataset. Same logic goes for titles, like Mr., Mrs., Ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, Download ner.csv from https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus\n",
    "ner_dataset = pd.read_csv(\"ner.csv\",encoding = \"ISO-8859-1\", error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ner_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset = ner_dataset.drop_duplicates()\n",
    "len(ner_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset[ner_dataset['sentence_idx']==13][['sentence_idx','word','tag','prev-word','prev-prev-word','next-word']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New entities - Title and Role\n",
    "\n",
    "- **Title**: Mr., Mrs., Professor, Doctor, ...\n",
    "- **Role**: President, Secretary General, U.N. Secretary, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick exploratory analysis of frequencies:\n",
    "- First PER token\n",
    "- Second PER token\n",
    "- First and second PER token\n",
    "- One before and first tokens of PER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate words before I-per\n",
    "bper = ner_dataset[ner_dataset['tag']=='B-per']\n",
    "bper_tokens = bper['word']\n",
    "prev_bper_token = bper['prev-word']\n",
    "next_bper_token = bper['next-word']\n",
    "two_prev_tokens = zip(prev_bper_token, bper_tokens)\n",
    "two_next_tokens = zip(bper_tokens, next_bper_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(\"20 most common PER token frequencies:\")\n",
    "Counter(bper_tokens).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"20 most common previous and first PER token frequencies:\")\n",
    "Counter(two_prev_tokens).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"20 most common first and second PER token frequencies:\")\n",
    "Counter(two_next_tokens).most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists of titles and roles to update as ttl, rol\n",
    "TITLES = ['Mr.','Ms.','Mrs.']\n",
    "ROLES = ['President','General','Senator','Secretary-General','Minister','General']\n",
    "BIGRAMS_ROLES = [('Prime','Minister'),('prime','minister'),('U.S.','President'),\n",
    "                 ('Venezuelan', 'President'),('Vice','President'), ('Foreign', 'Minister'),\n",
    "                 ('U.S.','Secretary'),('U.N.','Secretary'),('Defence','Secretary')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update title and per for most common cases\n",
    "\n",
    "def fix_bigram_title(df, row,index,first='Prime',second='Minister',tag='ttl'):\n",
    "    if row['word'] == first and row['next-word'] == second and 'per' in row['tag']:\n",
    "        df.loc[index,'tag'] = 'B-{}'.format(tag)\n",
    "    elif row['word'] == second and row['prev-word'] == first and 'per' in row['tag']:\n",
    "        df.loc[index,'tag'] = 'I-{}'.format(tag)\n",
    "    elif row['tag']== 'I-per' and row['prev-word'] == second and 'per' in row['tag']:\n",
    "        df.loc[index,'tag'] = 'B-per'\n",
    "\n",
    "def fix_unigram_title(df, prev_row,prev_index, row , index, title='President',tag='ttl'):\n",
    "    #print(row)\n",
    "    if prev_row['word'] == title and prev_row['tag'] == 'B-per' and row['tag']=='I-per':\n",
    "        df.loc[prev_index,'tag']='B-{}'.format(tag)\n",
    "        df.loc[index,'tag'] = 'B-per'\n",
    "\n",
    "prev_row = None\n",
    "prev_index = None\n",
    "for index, row in ner_dataset.iterrows():\n",
    "    # Handle 'Prime Minister'\n",
    "    for bigram in BIGRAMS_ROLES:\n",
    "        fix_bigram_title(ner_dataset,row,index,bigram[0],bigram[1],'rol')\n",
    "\n",
    "    if prev_row is not None:\n",
    "        for title in TITLES:\n",
    "            fix_unigram_title(df=ner_dataset,prev_row=prev_row,prev_index=prev_index,row=row,index=index,title=title,tag='ttl')\n",
    "        for role in ROLES:\n",
    "            fix_unigram_title(ner_dataset,prev_row,prev_index,row,index,role,'rol')\n",
    "\n",
    "    prev_row = row\n",
    "    prev_index = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dataset[ner_dataset['sentence_idx']==13][['sentence_idx','word','tag','prev-word','prev-prev-word','next-word']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only relevant columns\n",
    "dataset = ner_dataset[['sentence_idx','word','tag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv(\"../../../datasets/ner_with_titles.csv\",encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create templates base on NER dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "class SentenceGetter(object):\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        self.n_sent = 1\n",
    "        self.dataset = dataset\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t) for w,t in zip(s[\"word\"].values.tolist(),\n",
    "                                                        s[\"tag\"].values.tolist())]\n",
    "        self.grouped = self.dataset.groupby(\"sentence_idx\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @staticmethod    \n",
    "    def get_template(grouped,entity_name_replace_dict=None):\n",
    "        TAGS_TO_IGNORE = ['nat','eve','art','tim']\n",
    "        template = \"\"\n",
    "        i=0\n",
    "        cur_index = 0\n",
    "        ents = []\n",
    "        for token in grouped:\n",
    "            token_text = token[0].replace(\"[\", \"\").replace(\"]\",\"\")\n",
    "            token_tag = token[1]\n",
    "            if token_tag == 'O':\n",
    "                template += \" \" + token_text\n",
    "            elif 'B-' in token_tag and token_tag[2:] not in TAGS_TO_IGNORE:\n",
    "                if entity_name_replace_dict:\n",
    "                    ent = entity_name_replace_dict[token[1][2:]]\n",
    "                else:\n",
    "                    ent = token_tag[2:]\n",
    "                ents.append(ent)\n",
    "                template += \" [\" + ent + \"]\"\n",
    "        template = re.sub(r'\\s([?,\\':.!\"](?:|$))+', r'\\1', template)\n",
    "        \n",
    "        for ent in ents:\n",
    "            weird = \"[{}] [{}]\".format(ent,ent)\n",
    "            template = template.replace(weird,\"[{}]\".format(ent))\n",
    "        \n",
    "        #remove additional weird combinations:\n",
    "        \n",
    "        to_replace = {\n",
    "            \"[COUNTRY] [ROLE] [PERSON]\": \"[ROLE] [PERSON]\",\n",
    "            \"[COUNTRY] [ROLE]\" : \"[ROLE]\",\n",
    "            \"[ORGANIZATION] [ROLE] [PERSON]\" : \"[ORGANIZATION]'s [ROLE] [PERSON]\",\n",
    "            \"[COUNTRY] [LOCATION]\" : \"[LOCATION]\",\n",
    "            \"[LOCATION] [COUNTRY]\": \"[LOCATION]\",\n",
    "            \"[PERSON] [COUNTRY]\" : \"[PERSON]\",\n",
    "            \"[PERSON] [LOCATION]\" : \"[PERSON]\",\n",
    "            \"[COUNTRY] [PERSON]\" : \"[PERSON]\",\n",
    "            \"[LOCATION] [PERSON]\" : \"[PERSON]\",\n",
    "            \"The [ORGANIZATION]\" : \"[ORGANIZATION]\",\n",
    "            \"[PERSON] [ORGANIZATION]\" : \"[PERSON]\",\n",
    "            \"of [ORGANIZATION] [PERSON]\" : \"of [ORGANIZATION], [PERSON]\",\n",
    "            \"[ORGANIZATION] [PERSON]\" : \"[PERSON]\",\n",
    "            \"[PERSON] [PERSON]\": \"[PERSON]\",\n",
    "            \"[LOCATION] says\" : \"[PERSON] says\",\n",
    "            \"[LOCATION] said\" : \"[PERSON] said\"\n",
    "            \n",
    "            \n",
    "        }\n",
    "        \n",
    "        for weird in to_replace.keys():\n",
    "            template = template.replace(weird,to_replace[weird])\n",
    "        \n",
    "        return template.strip()\n",
    "    \n",
    "getter = SentenceGetter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITIES_DICTIONARY = {\"per\":\"PERSON\",\"gpe\":\"COUNTRY\",\"geo\":\"LOCATION\",\"org\":\"ORGANIZATION\",'ttl':'TITLE','rol':'ROLE'}\n",
    "\n",
    "sentences = getter.sentences\n",
    "print(\"original:\",sentences[12])\n",
    "print(\"template:\", getter.get_template(sentences[12],entity_name_replace_dict=ENTITIES_DICTIONARY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_templates = [SentenceGetter.get_template(sentence, ENTITIES_DICTIONARY) for sentence in sentences]\n",
    "new_templates[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file\n",
    "\n",
    "with open(\"../../presidio_evaluator/data_generator/raw_data/new_templates2.txt\",\"w+\", encoding = \"ISO-8859-1\") as f:\n",
    "    for template in new_templates:\n",
    "        f.write(\"%s\\n\" % template)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
