{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting PII (Personally Identifiable Information)\n",
    "\n",
    "**Using NLTK Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "import nltk, re\n",
    "import nltk.corpus\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, PunktSentenceTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords, state_union\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "from nltk import ne_chunk, pos_tag\n",
    "from nltk.tree import Tree\n",
    "from nltk import RegexpParser\n",
    "from nltk.chunk.api import ChunkParserI\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(palette='Set2', )\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from faker import Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Hello, you have called Virtual bank, this is Helen speaking. How may I help you?\n",
    "Hi Helen. I want to report a lost credit card.\n",
    "Okay. Do you have your Debit card number?\n",
    "Oh yes, that is 8574562111234522.\n",
    "Okay. That is 8574562111234522.\n",
    "Yes.\n",
    "What is your identification number?\n",
    "1145824598874.\n",
    "Okay, I have 1145824598874. And what is your name sir?\n",
    "My name is Sakai Jinn.\n",
    "Okay. Do you want me to permanent suspend your card sir?\n",
    "Yes, please. \n",
    "Okay, and your ledger balance in the account is 56,000 dollars, is that correct?\n",
    "Yes.\n",
    "Okay, I just permanent suspended your card. Thank you for using our service. Have a good day sir.\n",
    "Thank you.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, you have called Virtual bank, this is Helen speaking. How may I help you?\\nHi Helen. I want to report a lost credit card.\\nOkay. Do you have your Debit card number?\\nOh yes, that is 8574562111234522.\\nOkay. That is 8574562111234522.\\nYes.\\nWhat is your identification number?\\n1145824598874.\\nOkay, I have 1145824598874. And what is your name sir?\\nMy name is Sakai Jinn.\\nOkay. Do you want me to permanent suspend your card sir?\\nYes, please. \\nOkay, and your ledger balance in the account is 56,000 dollars, is that correct?\\nYes.\\nOkay, I just permanent suspended your card. Thank you for using our service. Have a good day sir.\\nThank you.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Stop words lists from nltk\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"don't\", \"you're\", 'again', \"she's\", 'no', 'can', \"needn't\", 'couldn', 'only', 'him', 'down', 'in', 'am', 'nor', 'aren', 'against', 'herself', 'didn', 'who', 'their', 'is', 'there', 'y', 'ma', 'a', \"aren't\", 'and', 'how', 'under', \"didn't\", 'yours', 'doing', \"weren't\", 'doesn', \"mightn't\", \"hadn't\", 'if', 'himself', 'do', \"mustn't\", 'theirs', 'm', 'on', 'you', 'itself', 'the', 'during', 'where', 'she', 'such', 'it', 'he', 'by', 've', \"doesn't\", 'weren', 'them', 'off', 'ourselves', 'me', 'why', 'of', 'before', 'until', 'each', 'hers', 'after', 'its', 'now', 'this', 'needn', \"you'll\", 'when', 'here', 'for', 's', 'more', 'an', 'are', 'both', 'themselves', 'up', \"it's\", 'but', 'other', 'll', 'don', 'been', 'yourselves', 'myself', 'through', \"should've\", 'further', 'does', 'hadn', 'very', 'did', \"couldn't\", 'because', 'wasn', 'were', 'our', \"that'll\", 'just', 'mustn', 'about', 'out', 'her', 'hasn', 'from', 'ours', 'then', 'most', 'not', 'any', \"you've\", \"haven't\", 'have', 'whom', 'we', 'into', 'below', 'some', \"shan't\", 'has', 'all', \"wasn't\", \"won't\", 'be', \"you'd\", 'than', 'which', 'will', 'was', 'between', 'above', 'i', 'while', 't', 'as', 'his', 'your', 'what', 'at', \"wouldn't\", 'being', 'few', 'won', 'isn', 'o', 'too', 'yourself', 'should', 'shan', \"hasn't\", 'once', 'so', \"shouldn't\", 're', 'haven', 'those', 'ain', 'd', 'having', 'mightn', 'they', 'my', 'over', 'to', 'had', 'same', 'shouldn', 'these', 'wouldn', \"isn't\", 'own', 'that', 'with', 'or'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\")) # Set checking is faster in Python than list.\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine stop words from nltk and json and puntuation together\n",
    "# Stopwords from stopwords-json\n",
    "stopwords_json = {\"en\": [\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\n",
    "                         \"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\n",
    "                         \"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\n",
    "                         \"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\n",
    "                         \"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\n",
    "                         \"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\n",
    "                         \"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\n",
    "                         \"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\n",
    "                         \"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\n",
    "                         \"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\n",
    "                         \"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\n",
    "                         \"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\n",
    "                         \"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\n",
    "                         \"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\n",
    "                         \"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\n",
    "                         \"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\n",
    "                         \"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\n",
    "                         \"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\n",
    "                         \"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\n",
    "                         \"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\n",
    "                         \"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\n",
    "                         \"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\n",
    "                         \"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\n",
    "                         \"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\n",
    "                         \"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\n",
    "                         \"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\n",
    "                         \"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\n",
    "                         \"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\n",
    "                         \"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\n",
    "                         \"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\n",
    "                         \"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\n",
    "                         \"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\n",
    "                         \"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\n",
    "                         \"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\n",
    "                         \"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\n",
    "                         \"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\n",
    "                         \"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\n",
    "                         \"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\n",
    "                         \"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\n",
    "                         \"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\n",
    "                         \"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\n",
    "                         \"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\n",
    "                         \"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\n",
    "                         \"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\n",
    "                         \"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\n",
    "                         \"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\n",
    "                         \"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\n",
    "                         \"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\n",
    "                         \"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]}\n",
    "\n",
    "stopwords_json_en = set(stopwords_json['en'])\n",
    "stopwords_nltk_en = set(stopwords.words('english'))\n",
    "stopwords_punct = set(punctuation)\n",
    "\n",
    "# Combine the stopwords.\n",
    "stoplist_combined = set.union(stopwords_json_en, stopwords_nltk_en, stopwords_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# Create new lemmatization function\n",
    "def penn2morphy(penntag):\n",
    "    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n",
    "    morphy_tag = {'NN': 'n', 'JJ': 'a',\n",
    "                  'VB': 'v', 'RB': 'r'}\n",
    "    try:\n",
    "        return morphy_tag[penntag[:2]]\n",
    "    except:\n",
    "        return 'n' # if mapping isn't found, fall back to Noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_sent(text): \n",
    "    # Text input is string, returns lowercased strings.\n",
    "    return [wnl.lemmatize(word.lower(), pos = penn2morphy(tag)) \n",
    "            for word, tag in pos_tag(word_tokenize(text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating removing stop words and lemmatize function\n",
    "def preprocess_text(text):\n",
    "    return [word for word in lemmatize_sent(text) \n",
    "            if word not in stoplist_combined]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence:\n",
      "['hello', ',', 'you', 'have', 'called', 'virtual', 'bank', ',', 'this', 'is', 'helen', 'speaking', '.', 'how', 'may', 'i', 'help', 'you', '?', 'hi', 'helen', '.', 'i', 'want', 'to', 'report', 'a', 'lost', 'credit', 'card', '.', 'okay', '.', 'do', 'you', 'have', 'your', 'debit', 'card', 'number', '?', 'oh', 'yes', ',', 'that', 'is', '8574562111234522', '.', 'okay', '.', 'that', 'is', '8574562111234522', '.', 'yes', '.', 'what', 'is', 'your', 'identification', 'number', '?', '1145824598874', '.', 'okay', ',', 'i', 'have', '1145824598874', '.', 'and', 'what', 'is', 'your', 'name', 'sir', '?', 'my', 'name', 'is', 'sakai', 'jinn', '.', 'okay', '.', 'do', 'you', 'want', 'me', 'to', 'permanent', 'suspend', 'your', 'card', 'sir', '?', 'yes', ',', 'please', '.', 'okay', ',', 'and', 'your', 'ledger', 'balance', 'in', 'the', 'account', 'is', '56,000', 'dollars', ',', 'is', 'that', 'correct', '?', 'yes', '.', 'okay', ',', 'i', 'just', 'permanent', 'suspended', 'your', 'card', '.', 'thank', 'you', 'for', 'using', 'our', 'service', '.', 'have', 'a', 'good', 'day', 'sir', '.', 'thank', 'you', '.'] \n",
      "\n",
      "Lemmatized and removed stopwords:\n",
      "['call', 'virtual', 'bank', 'helen', 'speak', 'helen', 'report', 'lose', 'credit', 'card', 'debit', 'card', 'number', '8574562111234522', '8574562111234522', 'identification', 'number', '1145824598874', '1145824598874', 'sir', 'sakai', 'jinni', 'permanent', 'suspend', 'card', 'sir', 'ledger', 'balance', 'account', '56,000', 'dollar', 'correct', 'permanent', 'suspend', 'card', 'service', 'good', 'day', 'sir']\n"
     ]
    }
   ],
   "source": [
    "# Try lemmatize_sent() and remove stopwords\n",
    "print('Original Sentence:')\n",
    "print(list(map(str.lower, word_tokenize(text))), '\\n')\n",
    "print('Lemmatized and removed stopwords:')\n",
    "print(preprocess_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating POS tagging function\n",
    "def postagging_word(text):\n",
    "    return nltk.pos_tag([word for word in lemmatize_sent(text) \n",
    "            if word not in stoplist_combined])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('call', 'VB'), ('virtual', 'JJ'), ('bank', 'NN'), ('helen', 'VBN'), ('speak', 'JJ'), ('helen', 'JJ'), ('report', 'NN'), ('lose', 'VB'), ('credit', 'NN'), ('card', 'NN'), ('debit', 'VBZ'), ('card', 'JJ'), ('number', 'NN'), ('8574562111234522', 'CD'), ('8574562111234522', 'CD'), ('identification', 'NN'), ('number', 'NN'), ('1145824598874', 'CD'), ('1145824598874', 'CD'), ('sir', 'NN'), ('sakai', 'NN'), ('jinni', 'NN'), ('permanent', 'JJ'), ('suspend', 'NN'), ('card', 'NN'), ('sir', 'NN'), ('ledger', 'JJR'), ('balance', 'NN'), ('account', 'NN'), ('56,000', 'CD'), ('dollar', 'NN'), ('correct', 'JJ'), ('permanent', 'JJ'), ('suspend', 'VB'), ('card', 'NN'), ('service', 'NN'), ('good', 'JJ'), ('day', 'NN'), ('sir', 'VB')]\n"
     ]
    }
   ],
   "source": [
    "print(postagging_word(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the entity names (NER)\n",
    "def get_chunks(text):\n",
    "    chunked = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "    prev = None \n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "    chunk_label = []\n",
    "\n",
    "    for i in chunked:\n",
    "        if type(i) == Tree:\n",
    "            current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "        elif current_chunk:\n",
    "            named_entity = \" \".join(current_chunk)\n",
    "            if named_entity not in continuous_chunk:\n",
    "                continuous_chunk.append(named_entity)\n",
    "                current_chunk = []\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return continuous_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Virtual', 'Helen', 'Okay', 'Okay Okay', 'Sakai Jinn', 'Okay Okay Okay']\n"
     ]
    }
   ],
   "source": [
    "print(get_chunks(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
