{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER Using 3 Models and Rules-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# others libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK and Stanford libraries\n",
    "import nltk, re, os\n",
    "import nltk.corpus\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, PunktSentenceTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "from nltk import ne_chunk, pos_tag\n",
    "from nltk.tree import Tree\n",
    "from nltk import RegexpParser\n",
    "from nltk.chunk.api import ChunkParserI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy libraries\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading json file and storing in data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_load(path):\n",
    "    # reading json file\n",
    "    with open(path, 'r') as json_file:\n",
    "        f = json.load(json_file)\n",
    "    data = f\n",
    "    \n",
    "    # Collecting index of word, word, start time, and end time\n",
    "    df = pd.DataFrame({'indx': ([X for X in range(len(data['values']['word']))]),\n",
    "                       'word': data['values']['word'], 'start_time': data['values']['start'],\n",
    "                       'end_time': data['values']['end']})\n",
    "    \n",
    "    df = df.set_index('indx')\n",
    "    \n",
    "    return data, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entities Recognizer Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanford NER Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stanford_pred(dictt, df):\n",
    "    \n",
    "    java_path = (\"C:/Program Files/Java/jdk-15.0.1/bin/java.exe\")\n",
    "    os.environ['JAVAHOME'] = java_path\n",
    "    jar = ('D:/Program/stanford-ner-4.0.0/stanford-ner.jar')\n",
    "    model = ('D:/Program/stanford-ner-4.0.0/classifiers/english.muc.7class.distsim.crf.ser') # 7 classes\n",
    "    st = StanfordNERTagger(model, jar, encoding = 'utf-8')\n",
    "    \n",
    "    word_token = word_tokenize(dictt)\n",
    "    classified_text = st.tag(word_token)\n",
    "\n",
    "    wordlst = []\n",
    "    ne_lst = []\n",
    "\n",
    "    for i in range(len(classified_text)):\n",
    "        if str(classified_text[i][1]) != 'O':\n",
    "            if str(classified_text[i][1]) == 'PERSON' or str(classified_text[i][1]) == 'LOCATION' or str(classified_text[i][1]) == 'ORGANIZATION' or str(classified_text[i][1]) == 'MONEY' or str(classified_text[i][1]) == 'DATE':\n",
    "                wordlst.append(str(classified_text[i][0]))\n",
    "                ne_lst.append(str(classified_text[i][1]))\n",
    "                \n",
    "    st_pred = []        \n",
    "    check = 0  \n",
    "\n",
    "    for ww in df['word']:\n",
    "        check = 0\n",
    "        for w, n in zip(wordlst, ne_lst):\n",
    "            if ww.__contains__(w):\n",
    "                check = 1\n",
    "                st_pred.append(str(n))\n",
    "                break\n",
    "        if check == 0:\n",
    "            st_pred.append('O')\n",
    "    \n",
    "    df['stanford_pred'] = st_pred\n",
    "    \n",
    "    return st_pred, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLTK_pred(dictt, df):\n",
    "    \n",
    "    word_token = word_tokenize(dictt)\n",
    "    tagged_words = pos_tag(word_token)\n",
    "    ne_tagged = ne_chunk(tagged_words, binary = False)\n",
    "\n",
    "    lst_word = []\n",
    "    lst_ne = []\n",
    "\n",
    "    for chunk in ne_tagged:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            if chunk.label() == 'PERSON' or chunk.label() == 'LOCATION' or chunk.label() == 'ORG' or chunk.label() == 'GPE' or chunk.label() == 'MONEY' or chunk.label() == 'DATE':\n",
    "                if chunk.label() == 'ORG':\n",
    "                    lst_word.append(chunk[0][0])\n",
    "                    lst_ne.append('ORGANIZATION')\n",
    "                if chunk.label() == 'LOC' or chunk.label() == 'GPE':\n",
    "                    lst_word.append(chunk[0][0])\n",
    "                    lst_ne.append('LOCATION')\n",
    "                else:\n",
    "                    lst_word.append(chunk[0][0])\n",
    "                    lst_ne.append(chunk.label())\n",
    "    \n",
    "    nltk_pred = []        \n",
    "    check = 0  \n",
    "\n",
    "    for ww in df['word']:\n",
    "        check = 0\n",
    "        for w, n in zip(lst_word, lst_ne):\n",
    "            if ww.__contains__(w):\n",
    "                check = 1\n",
    "                nltk_pred.append(str(n))\n",
    "                break\n",
    "        if check == 0:\n",
    "            nltk_pred.append('O')\n",
    "    \n",
    "    df['nltk_pred'] = nltk_pred\n",
    "    \n",
    "    return nltk_pred, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spaCy_pred(dictt, df):\n",
    "    \n",
    "    nlp = en_core_web_sm.load()\n",
    "    # list of words that have named entities\n",
    "    text = ([str(X) for X in nlp(dictt)\n",
    "            if (X.ent_type_ != '' and X.ent_type_ != 'CARDINAL' and X.ent_type_ != 'PRODUCT') & (str(X) != 'a') & (str(X) != 'good') & (str(X) != 'day') & (str(X) != '.') & (str(X) != ',')])\n",
    "    # list of named entities\n",
    "    ne = ([X.ent_type_ for X in nlp(dictt)\n",
    "            if (X.ent_type_ != '' and X.ent_type_ != 'CARDINAL' and X.ent_type_ != 'PRODUCT') & (str(X) != 'a') & (str(X) != 'good') & (str(X) != 'day') & (str(X) != '.') & (str(X) != ',')])\n",
    "    \n",
    "    sp_pred = []\n",
    "    \n",
    "    for n, i in enumerate(ne):\n",
    "        if i == 'LOC':\n",
    "            ne[n] = 'LOCATION'\n",
    "        if i == 'GPE':\n",
    "            ne[n] = 'LOCATION'\n",
    "        if i == 'ORG':\n",
    "            ne[n] = 'ORGANIZATION'\n",
    "          \n",
    "    check = 0  \n",
    "    \n",
    "    for ww in df['word']:\n",
    "        check = 0\n",
    "        for w, n in zip(text, ne):\n",
    "            if ww.__contains__(w):\n",
    "                check = 1\n",
    "                sp_pred.append(str(n))\n",
    "                break\n",
    "        if check == 0:\n",
    "            sp_pred.append('O')\n",
    "                \n",
    "    df['spacy_pred'] = sp_pred\n",
    "                \n",
    "    return sp_pred, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Real Named Entities and Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_models(df):\n",
    "    \n",
    "    # ------------ Selecting same named entity predictions 2 of 3 models ------------\n",
    "    \n",
    "    i_twooth = []\n",
    "    ne_twooth = []\n",
    "\n",
    "    for i, st, nl, sp in zip(df.index, df['stanford_pred'], df['nltk_pred'], df['spacy_pred']):\n",
    "        # check if spacy predict 2 NE\n",
    "        if (st == 'O' or nl == 'O') and (str(sp) == 'DATE' or str(sp) == 'PERSON'):\n",
    "            i_twooth.append(i)\n",
    "            ne_twooth.append(str(sp))\n",
    "        # check if stanford and nltk are same named entities\n",
    "        elif (st != 'O' and nl != 'O') and (str(st) == str(nl)):\n",
    "            i_twooth.append(i)\n",
    "            ne_twooth.append(str(st))\n",
    "        # check if stanford and spacy are same named entities\n",
    "        elif (st != 'O' and sp != 'O') and (str(st) == str(sp)):\n",
    "            i_twooth.append(i)\n",
    "            ne_twooth.append(str(sp))\n",
    "        # check if nltk and spacy are same named entities\n",
    "        elif (nl != 'O' and sp != 'O') and (str(nl) == str(sp)):\n",
    "            i_twooth.append(i)\n",
    "            ne_twooth.append(str(sp))\n",
    "        \n",
    "    combined = []\n",
    "    combined_check = 0\n",
    "        \n",
    "    for i in df.index:\n",
    "        combined_check = 0\n",
    "        for ii, n in zip(i_twooth, ne_twooth):\n",
    "            if i == ii:\n",
    "                combined_check = 1\n",
    "                combined.append(str(n))\n",
    "                break\n",
    "        if combined_check == 0:\n",
    "            combined.append('O')\n",
    "       \n",
    "    # ------------ Regular Expression checking ------------\n",
    "    \n",
    "    pii_index = []\n",
    "    pii_type = []\n",
    "    date_check = 0\n",
    "\n",
    "    for i, num in zip(df.index, df['word']):\n",
    "        date_check = 0\n",
    "        for ii in i_twooth:\n",
    "            if i == ii:\n",
    "                date_check = 1\n",
    "                break\n",
    "        if date_check == 0:\n",
    "            # ID card e.g. +666-666-666-6666\n",
    "            if re.search('(\\+?[0-9]{3,}-?[0-9]{3,}-?[0-9]{3,}-?[0-9]{4,})', num):\n",
    "                pii_index.append(i)\n",
    "                pii_type.append('PIINUM')\n",
    "            # phone number e.g. 666-666-6666\n",
    "            elif re.search('(\\+?[0-9]{3,}-?[0-9]{3,}-?[0-9]{4,})', num):\n",
    "                pii_index.append(i)\n",
    "                pii_type.append('PIINUM')\n",
    "            # account number e.g. 666-666-666\n",
    "            elif re.search('(\\+?[0-9]{3,}-?[0-9]{3,}-?[0-9]{3,})', num):\n",
    "                pii_index.append(i)\n",
    "                pii_type.append('PIINUM')\n",
    "            # card number\n",
    "            elif re.search('(\\+?[0-9]{2,}-?[0-9]{3,}-?[0-9]{3,}-?[0-9]+-?[0-9]+)', num):\n",
    "                pii_index.append(i)\n",
    "                pii_type.append('PIINUM')\n",
    "            # if not has punctuation\n",
    "            elif re.search('\\+?[0-9]{9,}', num):\n",
    "                pii_index.append(i)\n",
    "                pii_type.append('PIINUM')\n",
    "            \n",
    "    regex_lst = []\n",
    "    regex_check = 0\n",
    "        \n",
    "    for i in df.index:\n",
    "        regex_check = 0\n",
    "        for ii, pi in zip(pii_index, pii_type):\n",
    "            if i == ii:\n",
    "                regex_check = 1\n",
    "                regex_lst.append(str(pi))\n",
    "                break\n",
    "        if regex_check == 0:\n",
    "            regex_lst.append('O')\n",
    "\n",
    "    # ------------ Combining real ents and regex ------------\n",
    "            \n",
    "    cb_rg = []\n",
    "\n",
    "    for ent, regex in zip(combined, regex_lst):\n",
    "        if ent != 'O' and regex == 'O':\n",
    "            cb_rg.append(ent)\n",
    "        elif regex != 'O' and ent == 'O':\n",
    "            cb_rg.append(regex)\n",
    "        else:\n",
    "            cb_rg.append('O')\n",
    "            \n",
    "    df['real_ents'] = cb_rg\n",
    "    \n",
    "    return cb_rg, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating New Data Frame to Store Real Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_ents(df):\n",
    "    \n",
    "    formal_ents = df.drop(['stanford_pred', 'nltk_pred', 'spacy_pred'], axis = 1)\n",
    "    formal_ents = formal_ents[formal_ents['real_ents'] != 'O']\n",
    "    \n",
    "    return formal_ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
