{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting PII (Personally Identifiable Information)\n",
    "\n",
    "**Using NLTK Library**\n",
    "\n",
    "Ref: https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import nltk, re\n",
    "import nltk.corpus\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, PunktSentenceTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords, state_union\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "from nltk import ne_chunk, pos_tag\n",
    "from nltk.tree import Tree\n",
    "from nltk import RegexpParser\n",
    "from nltk.chunk.api import ChunkParserI\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(palette='Set2', )\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from faker import Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('D:/DSBA/Year3 Term2/Project/Datasets/conversation.json', 'r') as json_file:\n",
    "#     f = json_file.read()\n",
    "# data = json.loads(f)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in data['conversations']:\n",
    "#    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"Hello, you have called Virtual bank, this is Nancy speaking. How may I help you?\n",
    "Oh, I just had withdrawn some cash from the ATM machine and ATM transaction failed but money got debited. Can you fix this problem?\n",
    "Sure. What is your account number?\n",
    "It is 111236669.\n",
    "Just a moment …. Okay and what is your name ma’am?\n",
    "My name is Sandra Reed.\n",
    "Okay, Miss Reed. Can I have your identify number?\n",
    "Okay. 5589766523663.\n",
    "Okay. I have 5589766523663.\n",
    "Correct.\n",
    "Where is the ATM machine that you had withdraw the cash?\n",
    "I do not know where exactly it is, but it is in the Pattaya beach.\n",
    "That is fine, we will check your withdrawal transaction and we will refund the money to your account. Do you want to receive the message when we refunding the money?\n",
    "Yes, please.\n",
    "Okay, what is your phone number ma’am?\n",
    "8779526987.\n",
    "Okay, I have 8779526987. We will send the message when we refunding the money to your account.\n",
    "Thanks, Nancy.\n",
    "Have a good day ma’am. Thank you.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, you have called Virtual bank, this is Nancy speaking. How may I help you?\\nOh, I just had withdrawn some cash from the ATM machine and ATM transaction failed but money got debited. Can you fix this problem?\\nSure. What is your account number?\\nIt is 111236669.\\nJust a moment …. Okay and what is your name ma’am?\\nMy name is Sandra Reed.\\nOkay, Miss Reed. Can I have your identify number?\\nOkay. 5589766523663.\\nOkay. I have 5589766523663.\\nCorrect.\\nWhere is the ATM machine that you had withdraw the cash?\\nI do not know where exactly it is, but it is in the Pattaya beach.\\nThat is fine, we will check your withdrawal transaction and we will refund the money to your account. Do you want to receive the message when we refunding the money?\\nYes, please.\\nOkay, what is your phone number ma’am?\\n8779526987.\\nOkay, I have 8779526987. We will send the message when we refunding the money to your account.\\nThanks, Nancy.\\nHave a good day ma’am. Thank you.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "    Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentence is \n",
    "    called Tokenization. Token is a single entity that is building blocks for sentence or paragraph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sentences tokenization:** the process of splitting up strings into “sentences”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sent = sent_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello, you have called Virtual bank, this is Nancy speaking.',\n",
       " 'How may I help you?',\n",
       " 'Oh, I just had withdrawn some cash from the ATM machine and ATM transaction failed but money got debited.',\n",
       " 'Can you fix this problem?',\n",
       " 'Sure.',\n",
       " 'What is your account number?',\n",
       " 'It is 111236669.',\n",
       " 'Just a moment ….',\n",
       " 'Okay and what is your name ma’am?',\n",
       " 'My name is Sandra Reed.',\n",
       " 'Okay, Miss Reed.',\n",
       " 'Can I have your identify number?',\n",
       " 'Okay.',\n",
       " '5589766523663.',\n",
       " 'Okay.',\n",
       " 'I have 5589766523663.',\n",
       " 'Correct.',\n",
       " 'Where is the ATM machine that you had withdraw the cash?',\n",
       " 'I do not know where exactly it is, but it is in the Pattaya beach.',\n",
       " 'That is fine, we will check your withdrawal transaction and we will refund the money to your account.',\n",
       " 'Do you want to receive the message when we refunding the money?',\n",
       " 'Yes, please.',\n",
       " 'Okay, what is your phone number ma’am?',\n",
       " '8779526987.',\n",
       " 'Okay, I have 8779526987.',\n",
       " 'We will send the message when we refunding the money to your account.',\n",
       " 'Thanks, Nancy.',\n",
       " 'Have a good day ma’am.',\n",
       " 'Thank you.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Tokenization:** the process of splitting up “sentences” into “words”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Hello', ',', 'you', 'have', 'called', 'Virtual', 'bank', ',', 'this', 'is', 'Nancy', 'speaking', '.'], ['How', 'may', 'I', 'help', 'you', '?'], ['Oh', ',', 'I', 'just', 'had', 'withdrawn', 'some', 'cash', 'from', 'the', 'ATM', 'machine', 'and', 'ATM', 'transaction', 'failed', 'but', 'money', 'got', 'debited', '.'], ['Can', 'you', 'fix', 'this', 'problem', '?'], ['Sure', '.'], ['What', 'is', 'your', 'account', 'number', '?'], ['It', 'is', '111236669', '.'], ['Just', 'a', 'moment', '…', '.'], ['Okay', 'and', 'what', 'is', 'your', 'name', 'ma', '’', 'am', '?'], ['My', 'name', 'is', 'Sandra', 'Reed', '.'], ['Okay', ',', 'Miss', 'Reed', '.'], ['Can', 'I', 'have', 'your', 'identify', 'number', '?'], ['Okay', '.'], ['5589766523663', '.'], ['Okay', '.'], ['I', 'have', '5589766523663', '.'], ['Correct', '.'], ['Where', 'is', 'the', 'ATM', 'machine', 'that', 'you', 'had', 'withdraw', 'the', 'cash', '?'], ['I', 'do', 'not', 'know', 'where', 'exactly', 'it', 'is', ',', 'but', 'it', 'is', 'in', 'the', 'Pattaya', 'beach', '.'], ['That', 'is', 'fine', ',', 'we', 'will', 'check', 'your', 'withdrawal', 'transaction', 'and', 'we', 'will', 'refund', 'the', 'money', 'to', 'your', 'account', '.'], ['Do', 'you', 'want', 'to', 'receive', 'the', 'message', 'when', 'we', 'refunding', 'the', 'money', '?'], ['Yes', ',', 'please', '.'], ['Okay', ',', 'what', 'is', 'your', 'phone', 'number', 'ma', '’', 'am', '?'], ['8779526987', '.'], ['Okay', ',', 'I', 'have', '8779526987', '.'], ['We', 'will', 'send', 'the', 'message', 'when', 'we', 'refunding', 'the', 'money', 'to', 'your', 'account', '.'], ['Thanks', ',', 'Nancy', '.'], ['Have', 'a', 'good', 'day', 'ma', '’', 'am', '.'], ['Thank', 'you', '.']]\n"
     ]
    }
   ],
   "source": [
    "tokenized_word = []\n",
    "\n",
    "for sent in tokenized_sent:\n",
    "    tokenized_word.append(word_tokenize(sent))\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hello', ',', 'you', 'have', 'called', 'virtual', 'bank', ',', 'this', 'is', 'nancy', 'speaking', '.'], ['how', 'may', 'i', 'help', 'you', '?'], ['oh', ',', 'i', 'just', 'had', 'withdrawn', 'some', 'cash', 'from', 'the', 'atm', 'machine', 'and', 'atm', 'transaction', 'failed', 'but', 'money', 'got', 'debited', '.'], ['can', 'you', 'fix', 'this', 'problem', '?'], ['sure', '.'], ['what', 'is', 'your', 'account', 'number', '?'], ['it', 'is', '111236669', '.'], ['just', 'a', 'moment', '…', '.'], ['okay', 'and', 'what', 'is', 'your', 'name', 'ma', '’', 'am', '?'], ['my', 'name', 'is', 'sandra', 'reed', '.'], ['okay', ',', 'miss', 'reed', '.'], ['can', 'i', 'have', 'your', 'identify', 'number', '?'], ['okay', '.'], ['5589766523663', '.'], ['okay', '.'], ['i', 'have', '5589766523663', '.'], ['correct', '.'], ['where', 'is', 'the', 'atm', 'machine', 'that', 'you', 'had', 'withdraw', 'the', 'cash', '?'], ['i', 'do', 'not', 'know', 'where', 'exactly', 'it', 'is', ',', 'but', 'it', 'is', 'in', 'the', 'pattaya', 'beach', '.'], ['that', 'is', 'fine', ',', 'we', 'will', 'check', 'your', 'withdrawal', 'transaction', 'and', 'we', 'will', 'refund', 'the', 'money', 'to', 'your', 'account', '.'], ['do', 'you', 'want', 'to', 'receive', 'the', 'message', 'when', 'we', 'refunding', 'the', 'money', '?'], ['yes', ',', 'please', '.'], ['okay', ',', 'what', 'is', 'your', 'phone', 'number', 'ma', '’', 'am', '?'], ['8779526987', '.'], ['okay', ',', 'i', 'have', '8779526987', '.'], ['we', 'will', 'send', 'the', 'message', 'when', 'we', 'refunding', 'the', 'money', 'to', 'your', 'account', '.'], ['thanks', ',', 'nancy', '.'], ['have', 'a', 'good', 'day', 'ma', '’', 'am', '.'], ['thank', 'you', '.']]\n"
     ]
    }
   ],
   "source": [
    "tokenized_word_lower = []\n",
    "\n",
    "for sent in tokenized_sent:\n",
    "    tokenized_word_lower.append([word.lower() for word in word_tokenize(sent)])\n",
    "print(tokenized_word_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distinct words\n",
    "fdist = FreqDist(tokenized_word_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tokenized_word:\n",
    "    fdist[i.lower()]+=1\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking distinct tokens\n",
    "len(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fdist.most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Frequency Distribution Plot\n",
    "plt.figure(figsize = (20,7))\n",
    "fdist.plot(30, cumulative = False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words\n",
    "\n",
    "    Stopwords considered as noise in the text. Text may contain stop words such as is, am, are, this, a, an, the, etc.\n",
    "    \n",
    "    In NLTK for removing stopwords, you need to create a list of stopwords and filter out your list of tokens from these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing Stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sent = []\n",
    "\n",
    "for w in tokens_text:\n",
    "    if w not in stop_words:\n",
    "        filtered_sent.append(w)\n",
    "\n",
    "print(\"Tokenized Sentence: \", tokens_text)\n",
    "print(\"\")\n",
    "print(\"Filterd Sentence: \", filtered_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexicon Normalization\n",
    "\n",
    "    Lexicon normalization considers another type of noise in the text. For example, connection, connected, connecting word reduce to a common word \"connect\". It reduces derivationally related forms of a word to a common root word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmatization:** reduces words to their base word, which is linguistically correct lemmas. It transforms root word with the use of vocabulary and morphological analysis. Lemmatization is usually more sophisticated than stemming. Stemmer works on an individual word without knowledge of the context. For example, The word \"better\" has \"good\" as its lemma. This thing will miss by stemming because it requires a dictionary look-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()\n",
    "\n",
    "lemma_words = []\n",
    "\n",
    "for w in filtered_sent:\n",
    "    lemma_words.append(lem.lemmatize(w.lower(), \"v\"))\n",
    "    \n",
    "print(\"Filtered Sentence: \", filtered_sent)\n",
    "print(\"\")\n",
    "print(\"Lemmatized Sentence: \", lemma_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lexicon Normalization\n",
    "#performing stemming and Lemmatization\n",
    "lem = WordNetLemmatizer()\n",
    "stem = PorterStemmer()\n",
    "\n",
    "word = \"spoken\"\n",
    "print(\"Lemmatized Word: \", lem.lemmatize(word, \"v\"))\n",
    "print(\"Stemmed Word: \", stem.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging\n",
    "\n",
    "    The primary target of Part-of-Speech(POS) tagging is to identify the grammatical group of a given word. Whether it is a NOUN, PRONOUN,\n",
    "    ADJECTIVE, VERB, ADVERBS, etc. based on the context. POS Tagging looks for relationships within the sentence and assigns a corresponding tag\n",
    "    to the word.\n",
    "    \n",
    "Checking pos tag lists at https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.pos_tag(lemma_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_continuous_chunks(text):\n",
    "    chunked = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "    prev = None\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for i in chunked:\n",
    "        if type(i) == Tree:\n",
    "            current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "        elif current_chunk:\n",
    "            named_entity = \" \".join(current_chunk)\n",
    "            if named_entity not in continuous_chunk:\n",
    "                continuous_chunk.append(named_entity)\n",
    "                current_chunk = []\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "#    if continuous_chunk:\n",
    "#        named_entity = \" \".join(current_chunk)\n",
    "#        if named_entity not in continuous_chunk:\n",
    "#            continuous_chunk.append(named_entity)\n",
    "\n",
    "    return continuous_chunk\n",
    "\n",
    "print(get_continuous_chunks(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPE means geo-political entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in nltk.sent_tokenize(sentence):\n",
    "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "        if hasattr(chunk, 'label'):\n",
    "            print(chunk.label(), ' '.join(c[0] for c in chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ie_preprocess(document):\n",
    "    sentences = nltk.sent_tokenize(document) # sentence segmenter\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences] # word tokenizer\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences] # part-of-speech tagger\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##### Test Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
