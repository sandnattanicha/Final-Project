{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER Using 3 Models and Rules-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# others libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# NLTK and Stanford libraries\n",
    "import nltk, re, os\n",
    "import nltk.corpus\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, PunktSentenceTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "from nltk import ne_chunk, pos_tag\n",
    "from nltk.tree import Tree\n",
    "from nltk import RegexpParser\n",
    "from nltk.chunk.api import ChunkParserI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# spaCy libraries\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading json file and storing in data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_load(path):\n",
    "    # reading json file\n",
    "    with open(path, 'r') as json_file:\n",
    "        f = json.load(json_file)\n",
    "    data = f\n",
    "    \n",
    "    # Collecting index of word, word, start time, and end time\n",
    "    df = pd.DataFrame({'indx': ([X for X in range(len(data['values']['word']))]),\n",
    "                       'word': data['values']['word'], 'start_time': data['values']['start'],\n",
    "                       'end_time': data['values']['end']})\n",
    "    \n",
    "    df = df.set_index('indx')\n",
    "    \n",
    "    return data, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanford NER Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "It has 3 models\n",
    "\n",
    "* 3 classes model for recognizing locations, person, and organizations\n",
    "* 4 classes model for recognizing locations, person, organizations, and miscellaneous entities\n",
    "* 7 classes model for recognizing locations, person, organizations, times, money, percents, and dates\n",
    "\n",
    "In this project, we use 7 classes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Stanford_pred(dictt, df):\n",
    "    \n",
    "    java_path = (\"C:/Program Files/Java/jdk-15.0.1/bin/java.exe\")\n",
    "    os.environ['JAVAHOME'] = java_path\n",
    "    jar = ('D:/Program/stanford-ner-4.0.0/stanford-ner.jar')\n",
    "    model = ('D:/Program/stanford-ner-4.0.0/classifiers/english.muc.7class.distsim.crf.ser') # 7 classes\n",
    "    st = StanfordNERTagger(model, jar, encoding = 'utf-8')\n",
    "    \n",
    "    word_token = word_tokenize(dictt)\n",
    "    classified_text = st.tag(word_token)\n",
    "\n",
    "    wordlst = []\n",
    "    ne_lst = []\n",
    "\n",
    "    for i in range(len(classified_text)):\n",
    "        if str(classified_text[i][1]) != 'O':\n",
    "            if str(classified_text[i][1]) == 'PERSON' or str(classified_text[i][1]) == 'LOCATION' or str(classified_text[i][1]) == 'ORGANIZATION' or str(classified_text[i][1]) == 'MONEY' or str(classified_text[i][1]) == 'DATE':\n",
    "                wordlst.append(str(classified_text[i][0]))\n",
    "                ne_lst.append(str(classified_text[i][1]))\n",
    "                \n",
    "    st_pred = []        \n",
    "    check = 0  \n",
    "\n",
    "    for ww in df['word']:\n",
    "        check = 0\n",
    "        for w, n in zip(wordlst, ne_lst):\n",
    "            if ww.__contains__(w):\n",
    "                check = 1\n",
    "                st_pred.append(str(n))\n",
    "                break\n",
    "        if check == 0:\n",
    "            st_pred.append('O')\n",
    "    \n",
    "    df['stanford_pred'] = st_pred\n",
    "    \n",
    "    return st_pred, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "**NLTK recognizes the following entities:**\n",
    "* ORGANIZATION - Georgia-Pacific Corp., WHO\n",
    "* PERSON - Eddy Bonte, President Obama\n",
    "* LOCATION - Murray River, Mount Everest\n",
    "* DATE - June, 2008-06-29\n",
    "* TIME - two fifty a m, 1:30 p.m.\n",
    "* MONEY - 175 million Canadian Dollars, GBP 10.40\n",
    "* PERCENT - twenty pct, 18.75 %\n",
    "* FACILITY - Washington Monument, Stonehenge\n",
    "* GPE - South East Asia, Midlothian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLTK_pred(dictt, df):\n",
    "    \n",
    "    word_token = word_tokenize(dictt)\n",
    "    tagged_words = pos_tag(word_token)\n",
    "    ne_tagged = ne_chunk(tagged_words, binary = False)\n",
    "\n",
    "    lst_word = []\n",
    "    lst_ne = []\n",
    "\n",
    "    for chunk in ne_tagged:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            if chunk.label() == 'PERSON' or chunk.label() == 'LOCATION' or chunk.label() == 'ORG' or chunk.label() == 'GPE' or chunk.label() == 'MONEY' or chunk.label() == 'DATE':\n",
    "                if chunk.label() == 'ORG':\n",
    "                    lst_word.append(chunk[0][0])\n",
    "                    lst_ne.append('ORGANIZATION')\n",
    "                if chunk.label() == 'LOC' or chunk.label() == 'GPE':\n",
    "                    lst_word.append(chunk[0][0])\n",
    "                    lst_ne.append('LOCATION')\n",
    "                else:\n",
    "                    lst_word.append(chunk[0][0])\n",
    "                    lst_ne.append(chunk.label())\n",
    "    \n",
    "    nltk_pred = []        \n",
    "    check = 0  \n",
    "\n",
    "    for ww in df['word']:\n",
    "        check = 0\n",
    "        for w, n in zip(lst_word, lst_ne):\n",
    "            if ww.__contains__(w):\n",
    "                check = 1\n",
    "                nltk_pred.append(str(n))\n",
    "                break\n",
    "        if check == 0:\n",
    "            nltk_pred.append('O')\n",
    "    \n",
    "    df['nltk_pred'] = nltk_pred\n",
    "    \n",
    "    return nltk_pred, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "**spaCy recognizes the following entities:**\n",
    "* PERSON - People, including fictional.\n",
    "* NORP - Nationalities or religious or political groups.\n",
    "* FAC - Buildings, airports, highways, bridges, etc.\n",
    "* ORG - Companies, agencies, institutions, etc.\n",
    "* GPE - Countries, cities, states.\n",
    "* LOC - Non-GPE locations, mountain ranges, bodies of water.\n",
    "* PRODUCT - Objects, vehicles, foods, etc. (Not services.)\n",
    "* EVENT - Named hurricanes, battles, wars, sports events, etc.\n",
    "* WORK_OF_ART - Titles of books, songs, etc.\n",
    "* LAW - Named documents made into laws.\n",
    "* LANGUAGE - Any named language.\n",
    "* DATE - Absolute or relative dates or periods.\n",
    "* TIME - Times smaller than a day.\n",
    "* PERCENT - Percentage, including ”%“.\n",
    "* MONEY - Monetary values, including unit.\n",
    "* QUANTITY - Measurements, as of weight or distance.\n",
    "* ORDINAL - “first”, “second”, etc.\n",
    "* CARDINAL - Numerals that do not fall under another type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spaCy_pred(dictt, df):\n",
    "    \n",
    "    nlp = en_core_web_sm.load()\n",
    "    # list of words that have named entities\n",
    "    text = ([str(X) for X in nlp(dictt)\n",
    "            if (X.ent_type_ != '' and X.ent_type_ != 'CARDINAL') & (str(X) != 'a') & (str(X) != 'good') & (str(X) != 'day') & (str(X) != '.') & (str(X) != ',')])\n",
    "    # list of named entities\n",
    "    ne = ([X.ent_type_ for X in nlp(dictt)\n",
    "            if (X.ent_type_ != '' and X.ent_type_ != 'CARDINAL') & (str(X) != 'a') & (str(X) != 'good') & (str(X) != 'day') & (str(X) != '.') & (str(X) != ',')])\n",
    "    \n",
    "    sp_pred = []\n",
    "    \n",
    "    for n, i in enumerate(ne):\n",
    "        if i == 'LOC':\n",
    "            ne[n] = 'LOCATION'\n",
    "        if i == 'GPE':\n",
    "            ne[n] = 'LOCATION'\n",
    "        if i == 'ORG':\n",
    "            ne[n] = 'ORGANIZATION'\n",
    "          \n",
    "    check = 0  \n",
    "    \n",
    "    for ww in df['word']:\n",
    "        check = 0\n",
    "        for w, n in zip(text, ne):\n",
    "            if ww.__contains__(w):\n",
    "                check = 1\n",
    "                sp_pred.append(str(n))\n",
    "                break\n",
    "        if check == 0:\n",
    "            sp_pred.append('O')\n",
    "                \n",
    "    df['spacy_pred'] = sp_pred\n",
    "                \n",
    "    return sp_pred, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Real Named Entities and Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_models(df):\n",
    "    \n",
    "    # ------------ Selecting same named entity predictions 2 of 3 models ------------\n",
    "    \n",
    "    i_twooth = []\n",
    "    ne_twooth = []\n",
    "\n",
    "    for i, st, nl, sp in zip(df.index, df['stanford_pred'], df['nltk_pred'], df['spacy_pred']):\n",
    "        # check if stanford and nltk are same named entities\n",
    "        if (st != 'O' and nl != 'O') and (str(st) == str(nl)):\n",
    "            i_twooth.append(i)\n",
    "            ne_twooth.append(str(st))\n",
    "        # check if stanford and spacy are same named entities\n",
    "        elif (st != 'O' and sp != 'O') and (str(st) == str(sp)):\n",
    "            i_twooth.append(i)\n",
    "            ne_twooth.append(str(st))\n",
    "        # check if nltk and spacy are same named entities\n",
    "        elif (nl != 'O' and sp != 'O') and (str(nl) == str(sp)):\n",
    "            i_twooth.append(i)\n",
    "            ne_twooth.append(str(nl))\n",
    "        \n",
    "    combined = []\n",
    "    combined_check = 0\n",
    "        \n",
    "    for i in df.index:\n",
    "        combined_check = 0\n",
    "        for ii, n in zip(i_twooth, ne_twooth):\n",
    "            if i == ii:\n",
    "                combined_check = 1\n",
    "                combined.append(str(n))\n",
    "                break\n",
    "        if combined_check == 0:\n",
    "            combined.append('O')\n",
    "       \n",
    "    # ------------ Regular Expression checking ------------\n",
    "    \n",
    "    pii_index = []\n",
    "    pii_type = []\n",
    "    date_check = 0\n",
    "\n",
    "    for i, num in zip(df.index, df['word']):\n",
    "        date_check = 0\n",
    "        for ii in i_twooth:\n",
    "            if i == ii:\n",
    "                date_check = 1\n",
    "                break\n",
    "        if date_check == 0:\n",
    "            # ID card e.g. +666-666-666-6666\n",
    "            if re.search('(\\+?[0-9]{3,}-?[0-9]{3,}-?[0-9]{3,}-?[0-9]{4,})', num):\n",
    "                pii_index.append(i)\n",
    "                pii_type.append('IDCARD')\n",
    "            # phone number e.g. 666-666-6666\n",
    "            elif re.search('(\\+?[0-9]{3,}-?[0-9]{3,}-?[0-9]{4,})', num):\n",
    "                pii_index.append(i)\n",
    "                pii_type.append('PHONENUM')\n",
    "            # account number e.g. 666-666-666\n",
    "            elif re.search('(\\+?[0-9]{3,}-?[0-9]{3,}-?[0-9]{3,})', num):\n",
    "                pii_index.append(i)\n",
    "                pii_type.append('ACCNUM')\n",
    "            # card number\n",
    "            elif re.search('(\\+?[0-9]{2,}-?[0-9]{3,}-?[0-9]{3,}-?[0-9]+-?[0-9]+)', num):\n",
    "                pii_index.append(i)\n",
    "                pii_type.append('CARDNUM')\n",
    "            # if not has punctuation\n",
    "            elif re.search('\\+?[0-9]{9,}', num):\n",
    "                pii_index.append(i)\n",
    "                pii_type.append('PIINUM')\n",
    "            \n",
    "    regex_lst = []\n",
    "    regex_check = 0\n",
    "        \n",
    "    for i in df.index:\n",
    "        regex_check = 0\n",
    "        for ii, pi in zip(pii_index, pii_type):\n",
    "            if i == ii:\n",
    "                regex_check = 1\n",
    "                regex_lst.append(str(pi))\n",
    "                break\n",
    "        if regex_check == 0:\n",
    "            regex_lst.append('O')\n",
    "\n",
    "    # ------------ Combining real ents and regex ------------\n",
    "            \n",
    "    cb_rg = []\n",
    "\n",
    "    for ent, regex in zip(combined, regex_lst):\n",
    "        if ent != 'O' and regex == 'O':\n",
    "            cb_rg.append(ent)\n",
    "        elif regex != 'O' and ent == 'O':\n",
    "            cb_rg.append(regex)\n",
    "        else:\n",
    "            cb_rg.append('O')\n",
    "            \n",
    "    df['real_ents'] = cb_rg\n",
    "    \n",
    "    return cb_rg, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating New Data Frame to Store Real Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_ents(df):\n",
    "    \n",
    "    formal_ents = df.drop(['stanford_pred', 'nltk_pred', 'spacy_pred'], axis = 1)\n",
    "    formal_ents = formal_ents[formal_ents['real_ents'] != 'O']\n",
    "    \n",
    "    return formal_ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_label = pd.read_csv('D:/DSBA/Project/Final-Project-2/data/Text files/ref-nancy-sandra.csv')\n",
    "ref_label = [i for i in ref_label['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_pred, df = Stanford_pred(data['transcript'], df)\n",
    "nltk_pred, df = NLTK_pred(data['transcript'], df)\n",
    "sp_pred, df = spaCy_pred(data['transcript'], df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy of the 3 models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics.scores import accuracy\n",
    "st_acc = accuracy(ref_label, st_pred)\n",
    "nltk_acc = accuracy(ref_label, nltk_pred)\n",
    "spacy_acc = accuracy(ref_label, sp_pred)\n",
    "r_ent_acc = accuracy(ref_label, cb_rg)\n",
    "\n",
    "print('Stanford Accuracy: %.2f' % (st_acc * 100) + '%')\n",
    "print('NLTK Accuracy: %.2f' % (nltk_acc * 100) + '%')\n",
    "print('spaCy Accuracy: %.2f' % (spacy_acc * 100) + '%')\n",
    "print('\\n-------------------------------------------\\n')\n",
    "print('Combined Models and ReGex Accuracy: %.2f' % (r_ent_acc * 100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify named entity accuracy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_ENT(ent):\n",
    "    \n",
    "    person = []\n",
    "    org = []\n",
    "    loc = []\n",
    "    date = []\n",
    "    money = []\n",
    "    pii_num = []\n",
    "    \n",
    "    for p in ent:\n",
    "        if p != 'PERSON':\n",
    "            person.append('O')\n",
    "        else:\n",
    "            person.append(str(p))\n",
    "            \n",
    "    for o in ent:\n",
    "        if o != 'ORGANIZATION':\n",
    "            org.append('O')\n",
    "        else:\n",
    "            org.append(str(o))\n",
    "            \n",
    "    for l in ent:\n",
    "        if l != 'LOCATION':\n",
    "            loc.append('O')\n",
    "        else:\n",
    "            loc.append(str(l))\n",
    "            \n",
    "    for d in ent:\n",
    "        if d != 'DATE':\n",
    "            date.append('O')\n",
    "        else:\n",
    "            date.append(str(d))\n",
    "            \n",
    "    for m in ent:\n",
    "        if m != 'MONEY':\n",
    "            money.append('O')\n",
    "        else:\n",
    "            money.append(str(m))\n",
    "            \n",
    "    for pi in ent:\n",
    "        if pi != 'IDCARD' and pi != 'PHONENUM' and pi != 'ACCNUM' and pi != 'PIINUM':\n",
    "            pii_num.append('O')\n",
    "        else:\n",
    "            pii_num.append(str(pi))\n",
    "            \n",
    "    return person, org, loc, date, money, pii_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def acc_each_ENT(r_ps, r_org, r_loc, r_date, r_money, r_num, st_ps, st_org, st_loc, st_date, st_money, st_num, nltk_ps, nltk_org, nltk_loc, nltk_date, nltk_money, nltk_num, sp_ps, sp_org, sp_loc, sp_date, sp_money, sp_num, re_ps, re_org, re_loc, re_date, re_money, re_num):\n",
    "    \n",
    "    print('-------------------------------------------\\n')\n",
    "    print('PERSON DETECT ACCURACY:')\n",
    "    print('Stanford Accuracy: %.2f' % (accuracy(r_ps, st_ps) * 100) + '%')\n",
    "    print('NLTK Accuracy: %.2f' % (accuracy(r_ps, nltk_ps) * 100) + '%')\n",
    "    print('spaCy Accuracy: %.2f' % (accuracy(r_ps, sp_ps) * 100) + '%')\n",
    "    print('Combined Models and ReGex Accuracy: %.2f' % (accuracy(r_ps, re_ps) * 100) + '%')\n",
    "    print('\\n-------------------------------------------\\n')\n",
    "\n",
    "    print('ORGANIZATION DETECT ACCURACY:')\n",
    "    print('Stanford Accuracy: %.2f' % (accuracy(r_org, st_org) * 100) + '%')\n",
    "    print('NLTK Accuracy: %.2f' % (accuracy(r_org, nltk_org) * 100) + '%')\n",
    "    print('spaCy Accuracy: %.2f' % (accuracy(r_org, sp_org) * 100) + '%')\n",
    "    print('Combined Models and ReGex Accuracy: %.2f' % (accuracy(r_org, re_org) * 100) + '%')\n",
    "    print('\\n-------------------------------------------\\n')\n",
    "\n",
    "    print('LOCATION DETECT ACCURACY:')\n",
    "    print('Stanford Accuracy: %.2f' % (accuracy(r_loc, st_loc) * 100) + '%')\n",
    "    print('NLTK Accuracy: %.2f' % (accuracy(r_loc, nltk_loc) * 100) + '%')\n",
    "    print('spaCy Accuracy: %.2f' % (accuracy(r_loc, sp_loc) * 100) + '%')\n",
    "    print('Combined Models and ReGex Accuracy: %.2f' % (accuracy(r_loc, re_loc) * 100) + '%')\n",
    "    print('\\n-------------------------------------------\\n')\n",
    "\n",
    "    print('DATE DETECT ACCURACY:')\n",
    "    print('Stanford Accuracy: %.2f' % (accuracy(r_date, st_date) * 100) + '%')\n",
    "    print('NLTK Accuracy: %.2f' % (accuracy(r_date, nltk_date) * 100) + '%')\n",
    "    print('spaCy Accuracy: %.2f' % (accuracy(r_date, sp_date) * 100) + '%')\n",
    "    print('Combined Models and ReGex Accuracy: %.2f' % (accuracy(r_date, re_date) * 100) + '%')\n",
    "    print('\\n-------------------------------------------\\n')\n",
    "\n",
    "    print('MONEY DETECT ACCURACY:')\n",
    "    print('Stanford Accuracy: %.2f' % (accuracy(r_money, st_money) * 100) + '%')\n",
    "    print('NLTK Accuracy: %.2f' % (accuracy(r_money, nltk_money) * 100) + '%')\n",
    "    print('spaCy Accuracy: %.2f' % (accuracy(r_money, sp_money) * 100) + '%')\n",
    "    print('Combined Models and ReGex Accuracy: %.2f' % (accuracy(r_money, re_money) * 100) + '%')\n",
    "    print('\\n-------------------------------------------\\n')\n",
    "    \n",
    "    print('PII NUMBER DETECT ACCURACY:')\n",
    "    print('Stanford Accuracy: %.2f' % (accuracy(r_num, st_num) * 100) + '%')\n",
    "    print('NLTK Accuracy: %.2f' % (accuracy(r_num, nltk_num) * 100) + '%')\n",
    "    print('spaCy Accuracy: %.2f' % (accuracy(r_num, sp_num) * 100) + '%')\n",
    "    print('Combined Models and ReGex Accuracy: %.2f' % (accuracy(r_num, re_num) * 100) + '%')\n",
    "    print('\\n-------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracies of specific named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_ps, r_org, r_loc, r_date, r_money, r_num = only_ENT(ref_label)\n",
    "st_ps, st_org, st_loc, st_date, st_money, st_num = only_ENT(st_pred)\n",
    "nltk_ps, nltk_org, nltk_loc, nltk_date, nltk_money, nltk_num = only_ENT(nltk_pred)\n",
    "sp_ps, sp_org, sp_loc, sp_date, sp_money, sp_num = only_ENT(sp_pred)\n",
    "re_ps, re_org, re_loc, re_date, re_money, re_num = only_ENT(cb_rg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_each_ENT(r_ps, r_org, r_loc, r_date, r_money, r_num, st_ps, st_org, st_loc, st_date, st_money, st_num, nltk_ps, nltk_org, nltk_loc, nltk_date, nltk_money, nltk_num, sp_ps, sp_org, sp_loc, sp_date, sp_money, sp_num, re_ps, re_org, re_loc, re_date, re_money, re_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formal_ents.to_csv('D:/DSBA/Project/Final-Project-2/data/Text files/4_MODELS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb_rg, df = combined_models(df)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formal_ents = df.drop(['stanford_pred', 'nltk_pred', 'spacy_pred'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formal_ents = formal_ents[formal_ents['real_ents'] != 'O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formal_ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing more datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversation 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading json file and storing in df\n",
    "con2, df2 = read_load('D:/DSBA/Project/Final-Project-2/data/GG_Speech/conversation_2.json')\n",
    "\n",
    "# Stanford named entities prediction\n",
    "st_pred2, df2 = Stanford_pred(con2['transcript'], df2)\n",
    "# NLTK named entities prediction\n",
    "nltk_pred2, df2 = NLTK_pred(con2['transcript'], df2)\n",
    "# spaCy named entities prediction\n",
    "sp_pred2, df2 = spaCy_pred(con2['transcript'], df2)\n",
    "# Selecting same entities (2 of 3 models) and regular expressions (if they're pii num)\n",
    "cb_rg2, df2 = combined_models(df2)\n",
    "# Filtering the words that only have named entities\n",
    "formal_ents2 = filter_ents(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "formal_ents2.to_csv('D:/DSBA/Project/Final-Project-2/data/ner_pred/ner_pred_con2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversation 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading json file and storing in df\n",
    "con3, df3 = read_load('D:/DSBA/Project/Final-Project-2/data/GG_Speech/conversation_3.json')\n",
    "\n",
    "# Stanford named entities prediction\n",
    "st_pred3, df3 = Stanford_pred(con3['transcript'], df3)\n",
    "# NLTK named entities prediction\n",
    "nltk_pred3, df3 = NLTK_pred(con3['transcript'], df3)\n",
    "# spaCy named entities prediction\n",
    "sp_pred3, df3 = spaCy_pred(con3['transcript'], df3)\n",
    "# Selecting same entities (2 of 3 models) and regular expressions (if they're pii num)\n",
    "cb_rg3, df3 = combined_models(df3)\n",
    "# Filtering the words that only have named entities\n",
    "formal_ents3 = filter_ents(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "formal_ents3.to_csv('D:/DSBA/Project/Final-Project-2/data/ner_pred/ner_pred_con3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversation 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading json file and storing in df\n",
    "con15, df15 = read_load('D:/DSBA/Project/Final-Project-2/data/GG_Speech/conversation_15.json')\n",
    "\n",
    "# Stanford named entities prediction\n",
    "st_pred15, df15 = Stanford_pred(con15['transcript'], df15)\n",
    "# NLTK named entities prediction\n",
    "nltk_pred15, df15 = NLTK_pred(con15['transcript'], df15)\n",
    "# spaCy named entities prediction\n",
    "sp_pred15, df15 = spaCy_pred(con15['transcript'], df15)\n",
    "# Selecting same entities (2 of 3 models) and regular expressions (if they're pii num)\n",
    "cb_rg15, df15 = combined_models(df15)\n",
    "# Filtering the words that only have named entities\n",
    "formal_ents15 = filter_ents(df15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>real_ents</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indx</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Linda</td>\n",
       "      <td>3.4</td>\n",
       "      <td>3.8</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Linda.</td>\n",
       "      <td>6.7</td>\n",
       "      <td>7.2</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>544-770-7888.</td>\n",
       "      <td>14.5</td>\n",
       "      <td>18.2</td>\n",
       "      <td>PHONENUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>544-777-8088.</td>\n",
       "      <td>20.1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>PHONENUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Ashley</td>\n",
       "      <td>25.1</td>\n",
       "      <td>25.6</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>McMahon.</td>\n",
       "      <td>25.6</td>\n",
       "      <td>26.0</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Ashley</td>\n",
       "      <td>27.9</td>\n",
       "      <td>28.5</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>McMahon.</td>\n",
       "      <td>28.5</td>\n",
       "      <td>28.9</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>770-449-9999</td>\n",
       "      <td>32.7</td>\n",
       "      <td>36.2</td>\n",
       "      <td>PHONENUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>550-500-5559.</td>\n",
       "      <td>46.1</td>\n",
       "      <td>48.2</td>\n",
       "      <td>PHONENUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Troy</td>\n",
       "      <td>52.3</td>\n",
       "      <td>52.5</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>McMahon.</td>\n",
       "      <td>52.5</td>\n",
       "      <td>53.0</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>+512-960-500-6063.</td>\n",
       "      <td>57.4</td>\n",
       "      <td>61.2</td>\n",
       "      <td>IDCARD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>512-296-5663.</td>\n",
       "      <td>63.4</td>\n",
       "      <td>66.2</td>\n",
       "      <td>PHONENUM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    word  start_time  end_time real_ents\n",
       "indx                                                    \n",
       "8                  Linda         3.4       3.8    PERSON\n",
       "16                Linda.         6.7       7.2    PERSON\n",
       "38         544-770-7888.        14.5      18.2  PHONENUM\n",
       "42         544-777-8088.        20.1      23.0  PHONENUM\n",
       "49                Ashley        25.1      25.6    PERSON\n",
       "50              McMahon.        25.6      26.0    PERSON\n",
       "54                Ashley        27.9      28.5    PERSON\n",
       "55              McMahon.        28.5      28.9    PERSON\n",
       "64          770-449-9999        32.7      36.2  PHONENUM\n",
       "81         550-500-5559.        46.1      48.2  PHONENUM\n",
       "85                  Troy        52.3      52.5    PERSON\n",
       "86              McMahon.        52.5      53.0    PERSON\n",
       "97    +512-960-500-6063.        57.4      61.2    IDCARD\n",
       "101        512-296-5663.        63.4      66.2  PHONENUM"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formal_ents15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversation 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading json file and storing in df\n",
    "con18, df18 = read_load('D:/DSBA/Project/Final-Project-2/data/GG_Speech/conversation_18.json')\n",
    "\n",
    "# Stanford named entities prediction\n",
    "st_pred18, df18 = Stanford_pred(con18['transcript'], df18)\n",
    "# NLTK named entities prediction\n",
    "nltk_pred18, df18 = NLTK_pred(con18['transcript'], df18)\n",
    "# spaCy named entities prediction\n",
    "sp_pred18, df18 = spaCy_pred(con18['transcript'], df18)\n",
    "# Selecting same entities (2 of 3 models) and regular expressions (if they're pii num)\n",
    "cb_rg18, df18 = combined_models(df18)\n",
    "# Filtering the words that only have named entities\n",
    "formal_ents18 = filter_ents(df18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "formal_ents18.to_csv('D:/DSBA/Project/Final-Project-2/data/ner_pred/ner_pred_con18.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
